import operator
import traceback
from typing import List, Dict, Optional, Any
import torch
from langgraph.constants import START
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from typing_extensions import TypedDict
import json
import googlemaps

from tools.tool import extract_json_block, merge_dicts

REDUCER_METADATA = {"reducer": operator.add}

class TravelRequirements(TypedDict):
    destination: List[str]
    duration: Optional[int]
    attractions: List[str]
    hotel_preference: List[str]
    transport_preference: List[str]

class AgentState(TypedDict):
    messages: List[Any]
    current_step: str
    travel_requirements: Dict
    generated_plan: Optional[str]
    status: str

    @classmethod
    def create_initial(cls, input_message: str) -> Dict:
        return {
            "messages": [HumanMessage(content=input_message)],
            "current_step": "start",
            "travel_requirements": {
                "destination": [],
                "duration": None,
                "attractions": [],
                "hotel_preference": [],
                "transport_preference": []
            },
            "generated_plan": None,
            "status": "active"
        }


class ModelManager:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.use_finetuned = True
            cls._instance.qwen_tokenizer = AutoTokenizer.from_pretrained("../Qwen2.5-7B-Instruct")
            cls._instance.qwen_tokenizer.pad_token = cls._instance.qwen_tokenizer.eos_token

            cls._instance.base_model = AutoModelForCausalLM.from_pretrained(
                "../Qwen2.5-7B-Instruct",
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )

            cls._instance.finetuned_model = PeftModel.from_pretrained(cls._instance.base_model, "../lora-weights")
            cls._instance.finetuned_model.eval()

            # cls._instance.deepseek_tokenizer = AutoTokenizer.from_pretrained("../DeepSeek-R1-Distill-Qwen-7B")
            # cls._instance.deepseek_model = AutoModelForCausalLM.from_pretrained(
            #     "../DeepSeek-R1-Distill-Qwen-7B",
            #     torch_dtype=torch.bfloat16,
            #     device_map="auto"
            # )

        return cls._instance

    def generate_qwen_response(self, message: str) -> str:
        model = self.finetuned_model if self.use_finetuned else self.base_model

        full_prompt = f"<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n"
        inputs = self.qwen_tokenizer(full_prompt, return_tensors="pt", add_special_tokens=False)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=4096,
                temperature=0.7
            )

        response = self.qwen_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.split("<|im_end|>")[0].strip()

    # def generate_deepseek_response(self, message: str, history: List = None) -> str:
    #     if history is None:
    #         history = []
    #
    #     deepseek_tokenizer = AutoTokenizer.from_pretrained("./DeepSeek-R1-Distill-Qwen-7B")
    #
    #     deepseek_model = AutoModelForCausalLM.from_pretrained(
    #         "./DeepSeek-R1-Distill-Qwen-7B",
    #         torch_dtype=torch.bfloat16,
    #         device_map="auto"
    #     )
    #
    #     # æ„é€ å¯¹è¯å†å²
    #     full_prompt = ""
    #     for hist in history:
    #         full_prompt += f"<|im_start|>user\n{hist[0]}<|im_end|>\n<|im_start|>assistant\n{hist[1]}<|im_end|>\n"
    #     full_prompt += f"<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n"
    #
    #     inputs = deepseek_tokenizer(full_prompt, return_tensors="pt", add_special_tokens=False)
    #     inputs = {k: v.to(deepseek_model.device) for k, v in inputs.items()}
    #
    #     with torch.no_grad():
    #         outputs = deepseek_model.generate(
    #             **inputs,
    #             max_new_tokens=4096,
    #             temperature=0.7,
    #             top_p=0.9,
    #             repetition_penalty=1.1,
    #             do_sample=True,
    #             pad_token_id=deepseek_tokenizer.pad_token_id,
    #             eos_token_id=deepseek_tokenizer.encode("<|im_end|>", add_special_tokens=False)[0]
    #         )
    #
    #     response = deepseek_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)
    #     response = response.split("<|im_end|>")[0].strip()
    #     return response

class TravelKnowledgeBase:
    """ç©ºçš„çŸ¥è¯†åº“ç±»ï¼Œç”¨äºå ä½"""
    def get_attractions(self, destination: str) -> str:
        return "æœªé…ç½®æ™¯ç‚¹ä¿¡æ¯"

class TravelAgentNodes:
    def __init__(self, knowledge_base: 'TravelKnowledgeBase', gmaps_client: Optional['googlemaps.Client']):
        self.model_manager = ModelManager()
        self.knowledge_base = knowledge_base
        self.gmaps_client = gmaps_client
        self.use_gmaps = gmaps_client is not None and hasattr(gmaps_client, 'key') and gmaps_client.key != 'YOUR_GOOGLE_MAPS_API_KEY'
        self.use_knowledge_base = knowledge_base is not None and not isinstance(knowledge_base, type)

    def task_classifier(self, state: Dict) -> Dict:
        print("===== Starting task classifier =====")
        user_message = next((msg.content for msg in reversed(state["messages"])
                             if isinstance(msg, HumanMessage)), "")
        if state["status"] == "waiting-destination-active":
            special_prompt = "æˆ–è€…è¯·æ±‚ä¸­åŒ…å«å›½å®¶/åœ°åŒº/åŸå¸‚/åœ°ç‚¹/æ™¯ç‚¹åå­—"
        else:
            special_prompt = ""
        prompt = f"""ä½œä¸ºæ—…æ¸¸åŠ©æ‰‹ï¼Œä¸¥æ ¼åˆ¤æ–­ä»¥ä¸‹è¾“å…¥æ˜¯å¦æ˜¯æ—…æ¸¸è§„åˆ’ç›¸å…³çš„è¯·æ±‚ï¼Œå¹¶ä»ä»¥ä¸‹ä¸¤ç§å›å¤ä¸­é€‰æ‹©ï¼š
        å¦‚æœæ˜¯æ—…æ¸¸è§„åˆ’è¯·æ±‚{special_prompt}ï¼Œåªéœ€è¦å›å¤ä¸€ä¸ª'æ˜¯'å­—ï¼Œ
        å¦‚æœæ˜¯äº¤é€š/åœ°ç‚¹/é…’åº—/æ™¯ç‚¹æŸ¥è¯¢ï¼Œä¸æ˜¯æ—…æ¸¸è§„åˆ’è¯·æ±‚åˆ™åªå›å¤ä¸€ä¸ª'å¦'å­—ã€‚

        ç”¨æˆ·è¾“å…¥ï¼š{user_message}"""

        response = self.model_manager.generate_qwen_response(prompt)
        print("Raw classifier response:", response)

        next_step = 'requirement_analysis' if response.strip() == "æ˜¯" else 'general_query'
        classification_result = f"æ ¹æ®åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ª{'æ—…æ¸¸è§„åˆ’' if next_step == 'requirement_analysis' else 'æ™®é€šæŸ¥è¯¢'}è¯·æ±‚ğŸ˜ã€‚"

        new_messages = state["messages"] + [AIMessage(content=classification_result)]

        return {
            "messages": new_messages,
            "current_step": next_step,
            "travel_requirements": state["travel_requirements"].copy(),
            "generated_plan": state["generated_plan"],
            "status": "active"
        }

    def requirement_analyzer(self, state: Dict) -> Dict:
        print("=== Starting requirement analyze ===")
        user_message = next((msg.content for msg in reversed(state['messages']) if isinstance(msg, HumanMessage)),
                            None)

        prompt = f"""
        æ³¨æ„äº‹é¡¹ï¼š
        1. éœ€è¦æå–çš„ä¿¡æ¯çš„å­—æ®µåŠå…¶å®šä¹‰ï¼š
           destinationï¼ˆç›®çš„åœ°ï¼‰ï¼šå›½å®¶/åŸå¸‚/åœ°åŒºåç§°ï¼Œç”¨åˆ—è¡¨æ ¼å¼ã€‚
           durationï¼ˆå¤©æ•°ï¼‰ï¼šå¤©æ•°ï¼ˆä¸­æ–‡æ•°å­—éœ€è½¬ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—ï¼‰ã€‚
           attractionsï¼ˆæ™¯ç‚¹ï¼‰ï¼šæ™¯ç‚¹æˆ–åœ°ç‚¹åç§°ï¼Œç”¨åˆ—è¡¨æ ¼å¼ã€‚
           hotel_preferenceï¼ˆä½å®¿åå¥½ï¼‰ï¼šä½å®¿è¦æ±‚ï¼Œç”¨åˆ—è¡¨æ ¼å¼ã€‚
           transport_preferenceï¼ˆäº¤é€šåå¥½ï¼‰ï¼šäº¤é€šæ–¹å¼ï¼Œç”¨åˆ—è¡¨æ ¼å¼ã€‚
           ç¤ºä¾‹è¾“å…¥æ ¼å¼ï¼šæˆ‘æƒ³å»é‡åº†ç©3å¤©ï¼Œè¦å»è§£æ”¾ç¢‘å’Œæ´ªå´–æ´ï¼Œä½å®¿è¦æ±‚äº”æ˜Ÿçº§è±ªåé…’åº—ã€‚
           ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
           {{
              "destination": ["é‡åº†"],
              "duration": 3,
              "attractions": ["è§£æ”¾ç¢‘", "æ´ªå´–æ´"],
              "hotel_preference": ["äº”æ˜Ÿçº§è±ªåé…’åº—"],
              "transport_preference": []
            }}
        2. å­—æ®µæå–æ¡ä»¶ï¼š
           åªè¦æ˜¯å­—æ®µå®šä¹‰ç›¸å…³çš„ä¿¡æ¯éƒ½å¯ä»¥è¿›è¡Œæå–ï¼Œä¸ç®¡æ˜¯å¦å’Œæ—…æ¸¸æœ‰å…³ã€‚
           åªæœ‰æœªæ˜ç¡®æåˆ°çš„å­—æ®µéœ€è¦å¡«å……é»˜è®¤å€¼ï¼Œç¦æ­¢å¡«å……æ¨æµ‹å€¼ã€‚
        3. ä¸¥æ ¼ç¦æ­¢è¡Œä¸ºï¼š
           ç¦æ­¢æ¨æµ‹ç”¨æˆ·æ„å›¾ã€‚
           ç¦æ­¢è¡¥å……æœªæåŠçš„ä¿¡æ¯ã€‚
           ç¦æ­¢å¡«å……æ¨æµ‹å€¼ã€‚
           ç¦æ­¢è”æƒ³ç›¸å…³åœºæ™¯ã€‚
           ç¦æ­¢è¾“å‡ºé™¤JSONæ•°æ®ä»¥å¤–è¯­å¥ã€‚
           ç¦æ­¢åœ¨è¾“å‡ºçš„JSONæ•°æ®ä¸­åŠ å…¥//æ³¨é‡Šã€‚

        ä½ æ˜¯ä¸€ä¸ªä»ç”¨æˆ·è¾“å…¥ä¸­æå–ä¿¡æ¯çš„åŠ©æ‰‹ï¼Œéœ€è¦ä»ç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯æ•´åˆæˆæ»¡è¶³æ³¨æ„äº‹é¡¹çš„JSONæ ¼å¼æ•°æ®è¿›è¡Œä¸‹ä¸€æ­¥å¤„ç†ã€‚

        ç”¨æˆ·è¾“å…¥ï¼š{user_message}
        """

        response = self.model_manager.generate_qwen_response(prompt)
        print("Analyzer response:", response)
        response = extract_json_block(response)

        new_requirements = json.loads(response)
        requirements = state["travel_requirements"].copy()
        try:
            new_requirements = merge_dicts(requirements, new_requirements)
            next_step = "ask_destination" if not new_requirements.get("destination") else (
                "route_planning" if self.use_gmaps else "plan_generation"
            )

            def format_list(items):
                if not items:
                    return "æš‚æ— "
                if isinstance(items, list):
                    if len(items) == 1:
                        return items[0]
                    return "ã€".join(items[:-1]) + "å’Œ" + items[-1]
                else:
                    if isinstance(items, int):
                        return str(items) + "å¤©"
                    else:
                        return "æš‚æ— "

            destination = format_list(new_requirements.get('destination'))
            duration = format_list(new_requirements.get('duration'))
            attractions = format_list(new_requirements.get('attractions'))
            hotel = format_list(new_requirements.get('hotel_preference'))
            transport = format_list(new_requirements.get('transport_preference'))

            analysis_result = f"""æˆ‘å·²ç»åˆ†æäº†æ‚¨çš„éœ€æ±‚ğŸ¤—

            ğŸ¯ ç›®çš„åœ°ï¼š{destination if destination != "æš‚æ— " else "æœªæŒ‡å®šç›®çš„åœ°"}
            â±ï¸ è¡Œç¨‹å¤©æ•°ï¼š{duration if duration != "æš‚æ— " else "æœªæŒ‡å®šè¡Œç¨‹å¤©æ•°"}
            ğŸ° æ¨èæ™¯ç‚¹ï¼š{attractions if attractions != "æš‚æ— " else "æœªæŒ‡å®šå…·ä½“æ™¯ç‚¹"}
            ğŸ¨ ä½å®¿å®‰æ’ï¼š{hotel if hotel != "æš‚æ— " else "æœªæŒ‡å®šä½å®¿åå¥½"}
            ğŸš— äº¤é€šæ–¹å¼ï¼š{transport if transport != "æš‚æ— " else "æœªæŒ‡å®šäº¤é€šåå¥½"}
            """

        except json.JSONDecodeError:
            next_step = "error_handling"
            analysis_result = "æŠ±æ­‰ï¼Œåœ¨åˆ†ææ‚¨çš„éœ€æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"

        new_messages = state["messages"] + [AIMessage(content=analysis_result)]

        return {
            "messages": new_messages,
            "current_step": next_step,
            "travel_requirements": new_requirements,
            "generated_plan": state["generated_plan"],
            "status": state["status"]
        }

    def destination_asker(self, state: Dict) -> Dict:
        print("==== Starting destination asker ====")
        question = "æˆ‘æ³¨æ„åˆ°æ‚¨æ²¡æœ‰æŒ‡å®šç›®çš„åœ°ğŸ˜°ï¼Œè¯·é—®æ‚¨æƒ³å»å“ªä¸ªåœ°æ–¹æ—…æ¸¸å‘¢ğŸ¥°ï¼Ÿ"
        new_messages = state["messages"] + [AIMessage(content=question)]

        return {
            "messages": new_messages,
            "current_step": "wait_destination",
            "travel_requirements": state["travel_requirements"].copy(),
            "generated_plan": state["generated_plan"],
            "status": "waiting_destination_input"
        }

    def wait_destination_handler(self, state: Dict) -> Dict:
        print("=== Processing destination input ===")
        if state["status"] == "waiting_destination_input":
            return state

        user_input = state["messages"][-1].content
        new_requirements = state["travel_requirements"].copy()
        new_requirements["destination"] = user_input

        confirmation = f"å¥½çš„ï¼Œå·²ç»è®°å½•æ‚¨æƒ³å» {user_input} æ—…æ¸¸ã€‚è®©æˆ‘ç»§ç»­åˆ†æå…¶ä»–éœ€æ±‚ğŸ¥µã€‚"
        new_messages = state["messages"] + [AIMessage(content=confirmation)]

        return {
            "messages": new_messages,
            "current_step": "requirement_analysis",
            "travel_requirements": new_requirements,
            "generated_plan": state["generated_plan"],
            "status": "active"
        }

    def plan_generator(self, state: Dict) -> Dict:
        print("===== Starting plan generator ======")
        requirements = state["travel_requirements"]
        # attractions = (self.knowledge_base.get_attractions(requirements["destination"])
        #                if self.use_knowledge_base else "æœªæŒ‡å®šæ™¯ç‚¹")

        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„æ—…æ¸¸è®¡åˆ’ï¼Œå¯¹æ¯ä¸€å¤©çš„è¡Œç¨‹éƒ½è¦åšè¯¦ç»†çš„å®‰æ’ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼ŒæŒ‰ç…§æ—©ä¸­æ™šå®‰æ’ï¼Œ
        ä¹Ÿéœ€è¦è€ƒè™‘ç”¨æˆ·çš„ä½“åŠ›ï¼Œæ¾å¼›æœ‰åº¦ï¼Œè¡Œç¨‹è·¯çº¿åˆç†ï¼Œè¡Œç¨‹æœ€åè¾“å‡ºç”¨markdownæ ¼å¼ï¼Œå¯ä»¥åŠ å…¥emojiï¼š
        ç”¨æˆ·è¾“å…¥ï¼š
        ç›®çš„åœ°ï¼š{requirements.get('destination')}
        å¤©æ•°ï¼š{requirements.get('duration')}
        æ¨èæ™¯ç‚¹ï¼š{requirements.get('attractions')}
        ä½å®¿åå¥½ï¼š{requirements.get('hotel_preference')}
        äº¤é€šåå¥½ï¼š{requirements.get('transport_preference')}"""

        generated_plan = self.model_manager.generate_qwen_response(prompt)
        feedback_prompt = """\n\næ‚¨å¯¹è¿™ä»½è¡Œç¨‹å®‰æ’æ˜¯å¦æ»¡æ„å‘¢ï¼Ÿå¦‚æœéœ€è¦è°ƒæ•´ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“æƒ³è°ƒæ•´çš„åœ°æ–¹ï¼Œæˆ‘ä¼šé‡æ–°ä¸ºæ‚¨è§„åˆ’ã€‚å¦‚æœæ»¡æ„ï¼Œè¯·å›å¤"æ»¡æ„"â¤ï¸ã€‚"""

        new_messages = state["messages"] + [AIMessage(content=generated_plan + feedback_prompt)]

        return {
            "messages": new_messages,
            "current_step": "wait_feedback",
            "travel_requirements": requirements,
            "generated_plan": generated_plan,
            "status": "waiting_destination_input"
        }

    def wait_feedback_handler(self, state: Dict) -> Dict:
        print("====== Processing feedback ======")
        if state["status"] == "waiting_destination_input":
            return {
                **state,
                "current_step": END
            }

        user_feedback = state["messages"][-1].content

        if user_feedback.strip() == "æ»¡æ„":
            return {
                "messages": state["messages"] + [AIMessage(content="å¾ˆé«˜å…´æ‚¨æ»¡æ„è¿™ä»½è¡Œç¨‹å®‰æ’ï¼ç¥æ‚¨æ—…é€”æ„‰å¿«ğŸ˜Šï¼")],
                "current_step": END,
                "travel_requirements": state["travel_requirements"],
                "generated_plan": state["generated_plan"],
                "status": "active"
            }
        else:
            return {
                "messages": state["messages"] + [AIMessage(content="å¥½çš„ï¼Œæˆ‘ä¼šæ ¹æ®æ‚¨çš„åé¦ˆé‡æ–°è§„åˆ’è¡Œç¨‹ã€‚")],
                "current_step": "requirement_analysis",
                "travel_requirements": state["travel_requirements"],
                "generated_plan": state["generated_plan"],
                "status": "active"
            }

    def route_planner(self, state: Dict) -> Dict:
        print("====== Starting route planner ======")
        if not self.use_gmaps:
            return {
                "messages": state["messages"],
                "current_step": END,
                "travel_requirements": state["travel_requirements"].copy(),
                "generated_plan": state["generated_plan"],
                "status": state["status"]
            }

        try:
            directions = self.gmaps_client.directions(
                origin="å½“å‰ä½ç½®",
                destination=state["travel_requirements"]["destination"],
                mode="transit"
            )

            route_info = f"äº¤é€šè·¯çº¿ä¿¡æ¯ï¼š\n{directions}"
            new_messages = state["messages"] + [AIMessage(content=route_info)]
            next_step = END

        except Exception as e:
            error_message = f"åœ¨è·å–è·¯çº¿ä¿¡æ¯æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"
            new_messages = state["messages"] + [AIMessage(content=error_message)]
            next_step = "error_handling"

        return {
            "messages": new_messages,
            "current_step": next_step,
            "travel_requirements": state["travel_requirements"].copy(),
            "generated_plan": state["generated_plan"],
            "status": state["status"]
        }

    def error_handler(self, state: Dict) -> Dict:
        print("====== Starting error handler ======")
        error_message = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ğŸ˜­ğŸ˜­ğŸ˜­ã€‚"
        new_messages = state["messages"] + [AIMessage(content=error_message)]

        return {
            "messages": new_messages,
            "current_step": "task_classifier",
            "travel_requirements": {},
            "generated_plan": None,
            "status": "active"
        }

    def general_query_handler(self, state: Dict) -> Dict:
        print("========Starting general query handler========")
        user_message = state["messages"][-1].content

        dialogue_history = []
        for msg in state["messages"][:-1]:
            if isinstance(msg, HumanMessage):
                dialogue_history.append(f"ç”¨æˆ·: {msg.content}")
            elif isinstance(msg, AIMessage):
                dialogue_history.append(f"åŠ©æ‰‹: {msg.content}")

        history_text = "\n".join(dialogue_history)

        prompt = f"""ä½œä¸ºæ—…æ¸¸åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹å¯¹è¯å†å²å’Œå½“å‰æ—…æ¸¸éœ€æ±‚æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¯ä»¥ç”¨emojiï¼š

        å¯¹è¯å†å²ï¼š
        {history_text}

        å½“å‰æ—…æ¸¸éœ€æ±‚ï¼š
        ç›®çš„åœ°ï¼š{state['travel_requirements'].get('destination')}
        å¤©æ•°ï¼š{state['travel_requirements'].get('duration')}
        æ™¯ç‚¹ï¼š{state['travel_requirements'].get('attractions')}
        ä½å®¿ï¼š{state['travel_requirements'].get('hotel_preference')}
        äº¤é€šï¼š{state['travel_requirements'].get('transport_preference')}

        ç”¨æˆ·å½“å‰é—®é¢˜ï¼š{user_message}
        """

        response = self.model_manager.generate_qwen_response(prompt)
        new_messages = state["messages"] + [AIMessage(content=response)]

        return {
            "messages": new_messages,
            "current_step": END,
            "travel_requirements": state["travel_requirements"].copy(),
            "generated_plan": state["generated_plan"],
            "status": "after_general_query"
        }


class TravelAgent:
    def __init__(self, knowledge_base: Optional[TravelKnowledgeBase] = None,
                 gmaps_api_key: Optional[str] = None):
        self.knowledge_base = knowledge_base or TravelKnowledgeBase()
        self.gmaps_client = None if not gmaps_api_key or gmaps_api_key == 'YOUR_GOOGLE_MAPS_API_KEY' else googlemaps.Client(
            key=gmaps_api_key)
        self.model_manager = ModelManager()
        self.workflow = create_travel_agent_workflow(self.knowledge_base, self.gmaps_client)
        self.conversation_states = {}
        self.dialogue_histories = {}
        self.current_conversation_id = 0

    def reset(self):
        if self.current_conversation_id in self.conversation_states:
            del self.conversation_states[self.current_conversation_id]
        if self.current_conversation_id in self.dialogue_histories:
            del self.dialogue_histories[self.current_conversation_id]

    def set_conversation_id(self, conversation_id: int):
        self.current_conversation_id = conversation_id
        if conversation_id not in self.dialogue_histories:
            self.dialogue_histories[conversation_id] = []
        if conversation_id not in self.conversation_states:
            self.conversation_states[conversation_id] = None

    def process_input(self, user_input: str) -> List[tuple]:
        try:
            if self.current_conversation_id not in self.dialogue_histories:
                self.dialogue_histories[self.current_conversation_id] = []

            current_history = self.dialogue_histories[self.current_conversation_id]

            if self.conversation_states.get(self.current_conversation_id) is not None:
                new_messages = self.conversation_states[self.current_conversation_id]["messages"] + [
                    HumanMessage(content=user_input)]
                self.conversation_states[self.current_conversation_id] = {
                    "messages": new_messages,
                    "current_step": self.conversation_states[self.current_conversation_id]["current_step"],
                    "travel_requirements": self.conversation_states[self.current_conversation_id][
                        "travel_requirements"].copy(),
                    "generated_plan": self.conversation_states[self.current_conversation_id]["generated_plan"],
                    "status": "active"
                }
            else:
                self.conversation_states[self.current_conversation_id] = AgentState.create_initial(user_input)

            final_state = self.workflow.invoke(self.conversation_states[self.current_conversation_id])
            self.conversation_states[self.current_conversation_id] = final_state

            # åªå¤„ç†æ–°æ¶ˆæ¯
            dialogue_history = []
            for message in final_state["messages"]:
                if isinstance(message, HumanMessage):
                    dialogue_history.append(("user", message.content))
                elif isinstance(message, AIMessage):
                    dialogue_history.append(("assistant", message.content))

            # æ›´æ–°å½“å‰å¯¹è¯çš„å†å²è®°å½•
            self.dialogue_histories[self.current_conversation_id] = dialogue_history

            return dialogue_history

        except Exception as e:
            print(f"Error in process_input: {traceback.format_exc()}")
            error_history = [("user", user_input), ("assistant", f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")]
            self.dialogue_histories[self.current_conversation_id] = error_history
            return error_history


def create_travel_agent_workflow(knowledge_base: TravelKnowledgeBase,
                                 gmaps_client: Optional['googlemaps.Client']) -> StateGraph:
    workflow = StateGraph(AgentState)
    nodes = TravelAgentNodes(knowledge_base, gmaps_client)

    workflow.add_node("task_classifier", nodes.task_classifier)
    workflow.add_node("requirement_analysis", nodes.requirement_analyzer)
    workflow.add_node("ask_destination", nodes.destination_asker)
    workflow.add_node("wait_destination", nodes.wait_destination_handler)
    workflow.add_node("plan_generation", nodes.plan_generator)
    workflow.add_node("wait_feedback", nodes.wait_feedback_handler)
    workflow.add_node("route_planning", nodes.route_planner)
    workflow.add_node("error_handling", nodes.error_handler)
    workflow.add_node("general_query", nodes.general_query_handler)

    workflow.add_edge(START, "task_classifier")

    workflow.add_conditional_edges(
        "task_classifier",
        lambda x: x["current_step"],
        {
            "requirement_analysis": "requirement_analysis",
            "general_query": "general_query"
        }
    )

    workflow.add_conditional_edges(
        "requirement_analysis",
        lambda x: x["current_step"],
        {
            "ask_destination": "ask_destination",
            "plan_generation": "plan_generation",
            "route_planning": "route_planning",
            "error_handling": "error_handling"
        }
    )

    workflow.add_edge("ask_destination", "wait_destination")

    workflow.add_conditional_edges(
        "wait_destination",
        lambda x: x["status"],
        {
            "waiting_destination_input": END,
            "active": "requirement_analysis"
        }
    )

    workflow.add_edge("plan_generation", "wait_feedback")

    workflow.add_conditional_edges(
        "wait_feedback",
        lambda x: x["current_step"],
        {
            "requirement_analysis": "requirement_analysis",
            END: END
        }
    )

    workflow.add_conditional_edges(
        "route_planning",
        lambda x: x["current_step"],
        {
            "error_handling": "error_handling",
            END: END
        }
    )

    workflow.add_edge("error_handling", "task_classifier")

    workflow.add_edge("general_query", END)

    return workflow.compile()

def main():
    basic_agent = TravelAgent()

    print("æ¬¢è¿ä½¿ç”¨æ—…è¡ŒåŠ©æ‰‹ï¼è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ï¼Œè¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯ã€‚")

    while True:
        user_input = input("\n=========== HUMAN MESSAGE ===========\n")

        if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
            print("\n============ AI MESSAGE ============")
            print("æ„Ÿè°¢ä½¿ç”¨æ—…è¡ŒåŠ©æ‰‹ï¼Œå†è§ï¼")
            break

        try:
            dialogue_history = basic_agent.process_input(user_input)

            for role, content in dialogue_history:
                if role == "user":
                    print(f"\n=========== HUMAN MESSAGE ===========")
                    print(content)
                elif role == "assistant":
                    print(f"\n============ AI MESSAGE ============")
                    print(content)

        except Exception as e:
            print(f"\n============ AI MESSAGE ============")
            print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(traceback.format_exc())

if __name__ == "__main__":
    main()